{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **ChatBot Dictator**"
   ],
   "metadata": {
    "id": "GyqPD6LZyVHU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Microsoft/DialoGPT-medium model**\n",
    "**The microsoft/DialoGPT-medium model is a pre-trained conversational AI model based on the GPT architecture, developed by Microsoft. It was trained on a massive amount of text data from social media and internet forums, which has allowed it to learn a wide range of conversational patterns and styles.**\n",
    "\n",
    "**This model is medium-sized, which means it has fewer parameters and is less computationally intensive compared to larger models. However, it still has a large enough capacity to generate coherent and contextually appropriate responses.**\n",
    "\n",
    "**The model is designed to generate responses in a conversational manner, making it suitable for use in chatbots, virtual assistants, and other conversational AI applications. It has been trained to understand and respond to natural language input and can generate appropriate responses based on the context of the conversation.**\n",
    "\n",
    "**Overall, the microsoft/DialoGPT-medium model is a powerful tool for building conversational AI applications and has been used in a variety of real-world applications, including customer service chatbots, virtual assistants, and social media chatbots.**"
   ],
   "metadata": {
    "id": "J7g4XDa90dhS"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Reference**\n",
    "1. **Microsoft's official documentation on the model:** https://huggingface.co/microsoft/DialoGPT-medium\n",
    "\n",
    "2. **A research paper on the GPT architecture used in this model:** https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf\n",
    "\n",
    "3. **A Medium article discussing the model's architecture and capabilities:** https://towardsdatascience.com/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313\n",
    "\n",
    "4. **A case study of using the model for customer service chatbots:** https://medium.com/azure-data-lake/using-conversational-ai-to-improve-customer-service-ff1cc60d7b0b\n",
    "\n",
    "5. **A GitHub repository containing example code and projects using the model:** https://github.com/microsoft/DialoGPT"
   ],
   "metadata": {
    "id": "UNkVELIZ04pA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install transformers"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hyu_Ct50x5lU",
    "outputId": "0949a536-7928-48ed-9424-3f2bd1811410"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.8/6.8 MB\u001B[0m \u001B[31m47.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m7.8/7.8 MB\u001B[0m \u001B[31m83.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m200.1/200.1 KB\u001B[0m \u001B[31m21.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.27.4\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "86Vr0ku-xZ4K"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the tokenizer with left padding\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\", padding_side=\"left\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")"
   ],
   "metadata": {
    "id": "x4remT8bxrJF"
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Define a function for generating the bot's response to user input\n",
    "def generate_response(user_input, chat_history=\"\"):\n",
    "  try:\n",
    "        # Encode the new user input and concatenate with the chat history\n",
    "        new_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n",
    "        # Append the new user input to the chat history if there is one\n",
    "        if chat_history != \"\":\n",
    "            chat_history_ids = tokenizer.encode(chat_history, return_tensors='pt')\n",
    "            input_ids = torch.cat([chat_history_ids, new_input_ids], dim=-1)\n",
    "        else:\n",
    "            input_ids = new_input_ids\n",
    "\n",
    "        # Generate response by passing input_ids through the model\n",
    "        output = model.generate(input_ids=input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "        # Decode the model's output and extract the generated response\n",
    "        response = tokenizer.decode(output[:, input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "\n",
    "        # Return the response or a default response\n",
    "        if response.strip() != '':\n",
    "            return response\n",
    "        else:\n",
    "            return \"Sorry, I don't understand.\"\n",
    "  except Exception as e:\n",
    "    print(\"An error occurred while generating the bot's response:\")\n",
    "    print(str(e))\n",
    "    return \"Sorry, an error occurred while generating my response. Please try again later.\"\n",
    "\n",
    "# Start the conversation loop\n",
    "print(\"Hello! I am a Dictator. You can ask me anything.\")\n",
    "print(\"Enter quit if you want to stop the Conversation!!!! \")\n",
    "chat_history = \"\"\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input == \"quit\":\n",
    "      break \n",
    "    bot_response = generate_response(user_input, chat_history)\n",
    "    print(\"Bot: \" + bot_response)\n",
    "    chat_history += user_input + tokenizer.eos_token + bot_response + tokenizer.eos_token"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ElTmRbhNxrTD",
    "outputId": "fa101a44-28ed-4c94-eee8-46afc42d0204"
   },
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I am a Dictator. You can ask me anything.\n",
      "Enter quit if you want to stop the Conversation!!!! \n",
      "You: hi\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Hey! :D\n",
      "You: bonjour\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Bonjour! :D\n",
      "You: how are you doing today?\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: I'm doing well! How are you?\n",
      "You: great, who are you?\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: I'm a guy, but I'm a guy who likes girls.\n",
      "You: great I like girls too!! \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: I'm a guy who likes girls too!\n",
      "You: ok bye\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Bye bye\n",
      "You: quit\n"
     ]
    }
   ]
  }
 ]
}
